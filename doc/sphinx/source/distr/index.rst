.. distr/index: 

Distributed Training 
====================

Data scientists with locally scalable or cloud-based resources at their disposal 
may like to experiment with several modes for distributed training. The more 
data that is fed to a Deep Learning (DL) model, the “smarter” it gets; a natural 
result, then, is that the neural network model as a whole gets “smarter”.  
Likewise, one indicator of getting “smarter” is being able to make better use 
of fewer resources, something that is not possible with the traditional, 
GPU-based approach.   
